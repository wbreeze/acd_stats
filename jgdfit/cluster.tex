\subsection{Clustering for Chi-Square}

In order to apply the Chi-Square metric, we need to cluster grades.
The optimal method for doing this is the one that has least impact on
the mean and variance of the data.

The grade clusters must satisfy two constraints in order to get a goodness
of fit metric from the Chi-Square method.

First, there need to be at least six clusters. The number of degrees of freedom
for the Chi-Square metric is then, at minimum three--
six minus one, minus two for the estimated mean and standard deviation.
Three is the least number we believe will give a meaningful goodness of fit
test.

Second, there need to be at least six instances within each cluster. This is
the stricter criterion that leads to reduction in number of clusters.

Needing at minimum six clusters with six grades, in a uniform distribution
we would need thirty-six grades. The distributions are not uniform, so we
nearly double that number. We combine figure grades following the FPS ordering
of figures in order to produce a minimum of sixty grades per group.

The constraints at play here are:

\begin{enumerate}
\item{A minimum number of six grades in any cluster}
\item{Only adjacent clusters may be combined}
\item{Combine the least number}
\item{Have the weighted mean and variance close to the original}
\end{enumerate}

The clustering  method described by
Greenacre \cite{Greenacre}
and implemented by the
greenclust \cite{greenclust}
R package provides reorders the clusters.
We need to join only adjacent clusters, to maintain the order.

The algorithm of \cite{partitioning} and other algorithms for k-shape
ordered partitioning require a cumulative
function that measures the quality of each partitioning.
Here, we find that the mean and variance do not always decrease or
increase when joining two clusters.

Where $n$ is the number of grade values in the range of grades,
we explore the $2^{n-1}$ combinations of joins using the heuristic
of choosing to first try joining clusters with the smallest number of grades,
and a bound of minimum count of joins. The partition
is complete when no cluster contains less than six grades. If two solutions
have the same number of joins, we select the one with minimum of
$(\mu' - \mu) + (\sigma' - \sigma)$, in which
$\mu' - \mu$ is
the difference in the weighted mean, and
$\sigma' - \sigma$ is
the difference in the weighted variance.

In order to reduce the number of combinations to explore, we apply a
pre-processing step that combines strings of zero sized clusters into
one together with the lower numbered cluster neighboring the string.
Strings of zero sized clusters appear frequently in the data.
This pre-processing step
avoids having the algorithm try all combinations of zero sized clusters.
